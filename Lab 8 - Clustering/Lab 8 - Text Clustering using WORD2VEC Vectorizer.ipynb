{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5498e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nur\n",
      "[nltk_data]     Adilah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f63b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    " \"I enjoy hiking and camping in the mountains\",\n",
    " \"I like to read books and watch movies\",\n",
    " \"I prefer playing video games over sports\",\n",
    " \"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268c6b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bfbadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love playing football on the weekends',\n",
       " 'I enjoy hiking and camping in the mountains',\n",
       " 'I like to read books and watch movies',\n",
       " 'I prefer playing video games over sports',\n",
       " 'I love listening to music and going to concerts']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove punctuations in the documents\n",
    "def remove_punctuation(text):\n",
    "    # Initialize an empty string to store the result\n",
    "    punctuation_free = \"\"\n",
    "    \n",
    "    # Iterate over each character in the text\n",
    "    for i in text:\n",
    "        # Check if the character is not in the string.punctuation set\n",
    "        if i not in string.punctuation:\n",
    "            # If not, add the character to the result string\n",
    "            punctuation_free += i\n",
    "    return punctuation_free\n",
    "\n",
    "dataset = list(map(remove_punctuation, dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c406c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love playing football on the weekends',\n",
       " 'i enjoy hiking and camping in the mountains',\n",
       " 'i like to read books and watch movies',\n",
       " 'i prefer playing video games over sports',\n",
       " 'i love listening to music and going to concerts']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to standardize the cases in the documents into lower case\n",
    "dataset = list(map(str.lower,dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537a5679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'playing', 'football', 'on', 'the', 'weekends'],\n",
       " ['i', 'enjoy', 'hiking', 'and', 'camping', 'in', 'the', 'mountains'],\n",
       " ['i', 'like', 'to', 'read', 'books', 'and', 'watch', 'movies'],\n",
       " ['i', 'prefer', 'playing', 'video', 'games', 'over', 'sports'],\n",
       " ['i', 'love', 'listening', 'to', 'music', 'and', 'going', 'to', 'concerts']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = [doc.split() for doc in dataset]\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce0acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nur\n",
      "[nltk_data]     Adilah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Get the list of English stop words present in the library \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eefb185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'playing', 'football', 'weekends'],\n",
       " ['enjoy', 'hiking', 'camping', 'mountains'],\n",
       " ['like', 'read', 'books', 'watch', 'movies'],\n",
       " ['prefer', 'playing', 'video', 'games', 'sports'],\n",
       " ['love', 'listening', 'music', 'going', 'concerts']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if i not in stopwords:\n",
    "            output.append(i)\n",
    "    return output\n",
    "\n",
    "#Applying the remove_stopwords function to the 'token_data' column and storing the result in a new column 'clean_xstopwords'\n",
    "tokenized_dataset = list(map(remove_stopwords,tokenized_dataset))\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92531189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nur\n",
      "[nltk_data]     Adilah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'lov', 'play', 'footbal', 'on', 'the', 'weekend'], ['i', 'enjoy', 'hik', 'and', 'camp', 'in', 'the', 'mountain'], ['i', 'lik', 'to', 'read', 'book', 'and', 'watch', 'movy'], ['i', 'pref', 'play', 'video', 'gam', 'ov', 'sport'], ['i', 'lov', 'list', 'to', 'mu', 'and', 'going', 'to', 'concert']]\n",
      "[['i', 'love', 'playing', 'football', 'on', 'the', 'weekend'], ['i', 'enjoy', 'hiking', 'and', 'camping', 'in', 'the', 'mountain'], ['i', 'like', 'to', 'read', 'book', 'and', 'watch', 'movie'], ['i', 'prefer', 'playing', 'video', 'game', 'over', 'sport'], ['i', 'love', 'listening', 'to', 'music', 'and', 'going', 'to', 'concert']]\n"
     ]
    }
   ],
   "source": [
    "## Perform word stemming using Lancaster Stemmer in nltk librarY\n",
    "\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "#defining the object for stemming\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = []\n",
    "    for word in text:\n",
    "        stemmed_word = lancaster_stemmer.stem(word)\n",
    "        stem_text.append(stemmed_word)\n",
    "    return stem_text\n",
    "\n",
    "#applying the stemming function to the 'clean_xstopwords' column and storing the result in a new column 'clean_stemmed' \n",
    "stemmed_dataset = list(map(stemming,tokenized_dataset))\n",
    "stemmed_dataset\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "## Perform word lemmatization using WordNetLemmatizer( ) in nltk library\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#importing the Lemmatizer function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = []\n",
    "    for word in text:\n",
    "        lemmatized_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        lemm_text.append(lemmatized_word)\n",
    "    return lemm_text\n",
    "\n",
    "# #applying the lemmatizer function to the 'clean_xstopwords' column and storing the result in a new column 'clean_lemmatized1'\n",
    "dataset_lemm_stem = list(map(lemmatizer,stemmed_dataset))\n",
    "\n",
    "# #applying the lemmatizer function to the 'clean_stemmed' column and storing the result in a new column 'clean_lemmatized2'\n",
    "dataset_lemm = list(map(lemmatizer,tokenized_dataset))\n",
    "\n",
    "print(dataset_lemm_stem)\n",
    "\n",
    "print(dataset_lemm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d689fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=dataset_lemm, vector_size=100, \n",
    "window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9393ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in \n",
    "word2vec_model.wv], axis=0) for doc in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed309a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nur Adilah\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Nur Adilah\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "# Tabulate the document and predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53f990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
