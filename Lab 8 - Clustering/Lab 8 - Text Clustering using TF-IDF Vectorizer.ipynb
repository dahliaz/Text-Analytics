{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e4c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nur\n",
      "[nltk_data]     Adilah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bcd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    " \"I enjoy hiking and camping in the mountains\",\n",
    " \"I like to read books and watch movies\",\n",
    " \"I prefer playing video games over sports\",\n",
    " \"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2731b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8b35b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love playing football on the weekends',\n",
       " 'I enjoy hiking and camping in the mountains',\n",
       " 'I like to read books and watch movies',\n",
       " 'I prefer playing video games over sports',\n",
       " 'I love listening to music and going to concerts']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove punctuations in the documents\n",
    "def remove_punctuation(text):\n",
    "    # Initialize an empty string to store the result\n",
    "    punctuation_free = \"\"\n",
    "    \n",
    "    # Iterate over each character in the text\n",
    "    for i in text:\n",
    "        # Check if the character is not in the string.punctuation set\n",
    "        if i not in string.punctuation:\n",
    "            # If not, add the character to the result string\n",
    "            punctuation_free += i\n",
    "    return punctuation_free\n",
    "\n",
    "dataset = list(map(remove_punctuation, dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbef0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love playing football on the weekends',\n",
       " 'i enjoy hiking and camping in the mountains',\n",
       " 'i like to read books and watch movies',\n",
       " 'i prefer playing video games over sports',\n",
       " 'i love listening to music and going to concerts']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to standardize the cases in the documents into lower case\n",
    "dataset = list(map(str.lower,dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83f4f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'playing', 'football', 'on', 'the', 'weekends'],\n",
       " ['i', 'enjoy', 'hiking', 'and', 'camping', 'in', 'the', 'mountains'],\n",
       " ['i', 'like', 'to', 'read', 'books', 'and', 'watch', 'movies'],\n",
       " ['i', 'prefer', 'playing', 'video', 'games', 'over', 'sports'],\n",
       " ['i', 'love', 'listening', 'to', 'music', 'and', 'going', 'to', 'concerts']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = [doc.split() for doc in dataset]\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9398a70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nur\n",
      "[nltk_data]     Adilah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Get the list of English stop words present in the library \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5900ce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'playing', 'football', 'weekends'],\n",
       " ['enjoy', 'hiking', 'camping', 'mountains'],\n",
       " ['like', 'read', 'books', 'watch', 'movies'],\n",
       " ['prefer', 'playing', 'video', 'games', 'sports'],\n",
       " ['love', 'listening', 'music', 'going', 'concerts']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if i not in stopwords:\n",
    "            output.append(i)\n",
    "    return output\n",
    "\n",
    "#Applying the remove_stopwords function to the 'token_data' column and storing the result in a new column 'clean_xstopwords'\n",
    "tokenized_dataset = list(map(remove_stopwords,tokenized_dataset))\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74cce9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lov', 'play', 'footbal', 'weekend'], ['enjoy', 'hik', 'camp', 'mountain'], ['lik', 'read', 'book', 'watch', 'movy'], ['pref', 'play', 'video', 'gam', 'sport'], ['lov', 'list', 'mu', 'going', 'concert']]\n",
      "[['love', 'playing', 'football', 'weekend'], ['enjoy', 'hiking', 'camping', 'mountain'], ['like', 'read', 'book', 'watch', 'movie'], ['prefer', 'playing', 'video', 'game', 'sport'], ['love', 'listening', 'music', 'going', 'concert']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nur\n",
      "[nltk_data]     Adilah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Perform word stemming using Lancaster Stemmer in nltk librarY\n",
    "\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "#defining the object for stemming\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = []\n",
    "    for word in text:\n",
    "        stemmed_word = lancaster_stemmer.stem(word)\n",
    "        stem_text.append(stemmed_word)\n",
    "    return stem_text\n",
    "\n",
    "#applying the stemming function to the 'clean_xstopwords' column and storing the result in a new column 'clean_stemmed' \n",
    "stemmed_dataset = list(map(stemming,tokenized_dataset))\n",
    "stemmed_dataset\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "## Perform word lemmatization using WordNetLemmatizer( ) in nltk library\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#importing the Lemmatizer function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = []\n",
    "    for word in text:\n",
    "        lemmatized_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        lemm_text.append(lemmatized_word)\n",
    "    return lemm_text\n",
    "\n",
    "# #applying the lemmatizer function to the 'clean_xstopwords' column and storing the result in a new column 'clean_lemmatized1'\n",
    "dataset_lemm_stem = list(map(lemmatizer,stemmed_dataset))\n",
    "\n",
    "# #applying the lemmatizer function to the 'clean_stemmed' column and storing the result in a new column 'clean_lemmatized2'\n",
    "dataset_lemm = list(map(lemmatizer,tokenized_dataset))\n",
    "\n",
    "print(dataset_lemm_stem)\n",
    "\n",
    "print(dataset_lemm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b45cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset_lemm_stem is a list of lists\n",
    "dataset_lemm_concatenated = [' '.join(words) for words in dataset_lemm]\n",
    "\n",
    "# Now, use the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset_lemm_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad438860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "i love playing football on the weekends                            1\n",
      "i enjoy hiking and camping in the mountains                        0\n",
      "i like to read books and watch movies                              0\n",
      "i prefer playing video games over sports                           0\n",
      "i love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " camping\n",
      " mountain\n",
      " hiking\n",
      " enjoy\n",
      " video\n",
      " sport\n",
      " prefer\n",
      " game\n",
      " book\n",
      " read\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " football\n",
      " weekend\n",
      " going\n",
      " music\n",
      " concert\n",
      " listening\n",
      " playing\n",
      " sport\n",
      " camping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b3d5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
